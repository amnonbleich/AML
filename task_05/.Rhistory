install.packages("devtools")
install.packages("devtools")
source("https://bioconductor.org/biocLite.R")  # Install Bioconductor
biocLite(c("Biostrings","iterators","ade4"))                                     # Install required packages
devtools::install_github("crarlus/paprbag") # install paprbag
library(paprbag)
library(randomForest)
install.packages("randomForest")
library(randomForest)
data('trainigData')
library(paprbag)
data('trainigData')
data('trainingData')
set.seed(42)  # set seed to get always the same random numbers
postive_samples_positions<-sample.int(100,50)       #get 50 randomly chosen postiv labled datapoints
negative_samples_positions<-sample.int(100,50)+100  #get 50 randomly chosen negativ labled datapoints
trainingselection<- rep(F,200)  # Creats a vector with 200 FALSE entries
trainingselection[postive_samples_positions]<-TRUE
trainingselection[negative_samples_positions]<-TRUE
data_training<-trainingData[trainingselection,]
data_test<-trainingData[!trainingselection,]
trainingData[-1,]
View(trainingData[-1,])
View(trainingData[,-1])
View(trainingData)
rf <- randomForest(data_training$label ~ ., data=data_training[,-1])
rf <- randomForest(label ~ ., data=data_training[,-1])
rf <- randomForest(label ~ ., data=data_training)
View(data_training)
rf <- randomForest(Labels ~ ., data=data_training)
print(rf$confusion)
print(rf)
print (rf_model$confusion)
rf_model <- randomForest(Labels ~ ., data=data_training)
print (rf_model$confusion)
model_test <- predict(rf_model,data=data_test)
model_test
library(sensitivity)
install.packages("sensitivity")
library(sensitivity)
sensiitivity(data=model_test,reference=data_test[,1])
sensitivity(data=model_test,reference=data_test[,1])
sensitivity::sensitivity
sensitivity
install.packages("sensitivity")
install.packages("sensitivity")
librar(caret)
library(caret)
install.packages("caret")
library(caret)
print(rf_model)
rf_model <- randomForest(Labels ~ ., data=data_training)
library(paprbag)
library(randomForest)
library(paprbag)
library(randomForest)
data('trainingData')
set.seed(42)  # set seed to get always the same random numbers
postive_samples_positions<-sample.int(100,50)       #get 50 randomly chosen postiv labled datapoints
negative_samples_positions<-sample.int(100,50)+100  #get 50 randomly chosen negativ labled datapoints
trainingselection<- rep(F,200)  # Creats a vector with 200 FALSE entries
trainingselection[postive_samples_positions]<-TRUE
trainingselection[negative_samples_positions]<-TRUE
data_training<-trainingData[trainingselection,]
data_test<-trainingData[!trainingselection,]
rf_model <- randomForest(Labels ~ ., data=data_training)
print (rf_model$confusion)
rf_model
library(caret)
caret::confusionMatrix
caret::confusionMatrix(1,1)
install.packages("caret",
repos = "http://cran.r-project.org",
dependencies = c("Depends", "Imports", "Suggests"))
print (rf_model$confusion)
rf_model$proximity
rf_model$test
?randomForest
rf_model <- randomForest(Labels ~ ., data=data_training,xtest=data_test)
rf_model <- randomForest(Labels ~ ., data=data_training,xtest=data_training)
rf_model <- randomForest(Labels ~ ., data=data_training,subset = trainingselection)
rf_model <- randomForest(Labels ~ ., data=data_training,subset = postive_samples_positions)
rf_model$test
rf_model$confusion
print (rf_model$confusion)
rf_model <- randomForest(Labels ~ ., data=data_training)
rf_model
data_test[,-1]
View(data_test[,-1])
rf_model <- randomForest(Labels ~ ., data=data_training,xtest=data_test[,-1],ytest=data_test[,1])
rf_model$test
print(rf_model$test)
print(rf_model)
rf_model$predicted
rf_model$y
print(rf_model)
print(rf_model$confusion)
print(rf_model$test)
print(rf_model$err.rate)
print(rf_model$localImportance)
print(rf_model$test)
print(rf_model)
rf_model$test$confusion
print("asdf",3)
mean(c(0.24,0.36))
print (paste("Confusion",rf_model$confusion))
rf_model$test$confusion$class.error
rf_model$test$confusion[,3]
print("Training Set")
print(paste("Training set error:",mean(rf_model$confusion[,3])))
print(rf_model$confusion)
print("Test Set")
print(paste("Test set error:",mean(rf_model$test$confusion[,3])))
print(rf_model$confusion)
message("asdf")
print_results <- function (x)
{
print("Training Set")
print(paste("Training set error:",mean(x$confusion[,3])))
print(x$confusion)
print("Test Set")
print(paste("Test set error:",mean(x$test$confusion[,3])))
print(x$confusion)
}
print_results(rf_model)
string(1)
as.string(2)
as.character(2)
cat(2)
print_results <- function (x)
{
cat("Training Set")
cat(paste("Training set error:",mean(x$confusion[,3])))
cat(x$confusion)
cat("Test Set")
cat(paste("Test set error:",mean(x$test$confusion[,3])))
cat(x$confusion)
}
print_results(rf_model)
print_results <- function (x)
{
cat("Training Set\n")
cat(paste("Training set error:",mean(x$confusion[,3],'\n')))
print(x$confusion)
cat("Test Set")
cat(paste("Test set error:",mean(x$test$confusion[,3])))
print(x$confusion)
}
print_results(rf_model)
cat("Training Set\n")
cat(paste("Training set error:"))
print_results <- function (x)
{
cat("Training Set\n")
cat(paste("Training set error:",mean(x$confusion[,3],'\n')))
print(x$confusion)
cat("Test Set")
cat(paste("Test set error:",mean(x$test$confusion[,3])))
print(x$confusion)
}
print_results(rf_model)
rf_model$confusion[,3]
#install.packages("seqinr")
setwd("./Uni/Master/aml/AML/task_05/")
library(seqinr)# for loading fasta
#source("https://bioconductor.org/biocLite.R")
#biocLite("kebabs")
library(kebabs)
library(e1071)
####################
#     Task 1       #
####################
# We decided to use the  PUM2 dataset and the kebabs package. kernlab crashes caused by allocation errors
###
# This function removes all lower capitals from a given string in a list
###
stripe_upper<-function(element)
{
return(gsub('[[:lower:]]','',element[[1]]))
}
###
# This function give a string in a list back as lower letter string
###
stripe_all<-function(element)
{
return(tolower(element[[1]]))
}
###
# Load the positive sequences as vector of strings
###
PUM2pos <- read.fasta('./rna-binding/positive_PUM2.fasta',as.string=T,forceDNAtolower=F)
PUM2pos_striped <-sapply(PUM2pos,stripe_upper)  # only Upper letters
PUM2pos_all <- sapply(PUM2pos,stripe_all)       # full string aka binding site with flanks
###
# Load the negative sequences as vector of strings
###
PUM2neg <- read.fasta('./rna-binding/negative_PUM2.fasta', as.string=T,forceDNAtolower=F)
PUM2neg_striped <- sapply(PUM2neg,stripe_upper) # only Upper letters
PUM2neg_all <- sapply(PUM2neg,stripe_all)       # full string aka binding site with flanks
###
# create a dataframe containing the lable and the binding site sequence. Sequence is a DNAStringSet as required for the ksvm.
###
dataset_wo_flanks<-  data.frame( lables=as.factor(c(rep(T,length(PUM2pos_striped)),rep(F,length(PUM2neg_striped)))))
dataset_wo_flanks$seq<- DNAStringSet(c(PUM2pos_striped,PUM2neg_striped))
###
# create a dataframe containing the lable and the binding-site-sequence with the flanks. Sequence is a DNAStringSet as required for the ksvm.
###
dataset_flanks<-  data.frame( lables=as.factor(c(rep(T,length(PUM2pos_striped)),rep(F,length(PUM2neg_striped)))))
dataset_flanks$seq<- DNAStringSet(c(PUM2pos_all,PUM2neg_all))
####################
#     Task 2       #
####################
# We use the kebabs package
specK <- spectrumKernel(k=3) # create a spectrum kernel with kmer size 3
###
# Train the SVM with a subset of datapoints kmer size 3 and default costs
###
basic_model <- kbsvm(
x=dataset_wo_flanks$seq[-(10000:11692)],
y=dataset_wo_flanks$lables[-(10000:11692)],
kernel=specK,
pkg="e1071",
svm="C-svc")
# predict the rest of the datapoints with the trained svm
pred<- predict(model_init,dataset_wo_flanks$seq[(10000:11692)])
pred<- predict(basic_model,dataset_wo_flanks$seq[(10000:11692)])
acc<-list() # stores the accuracy for the different k
acc_min<-3  # just for axis limits for the plot
acc_max<-0  # just for axis limits for the plot
# AUC
auc<-list() # stores the area under the curve for the different k
auc_min<-3  # just for axis limits for the plot
auc_max<-0  # just for axis limits for the plot
ks<-c(3,4,5)  # the different kmer sizes we tried out
costs<-c(0.5,0.75,1,2,6) # the different costs we use
for (k in ks) # for each kmer size
{
accC<-c()
aucC<-c()
for (cost in costs)
{
cat(paste(k,'-',cost,'\n'))
specK <- spectrumKernel(k=k)
model <- kbsvm(
x=dataset_wo_flanks$seq,
y=dataset_wo_flanks$lables,
kernel=specK,
pkg="e1071",
svm="C-svc",
perfParameters=c('ACC','AUC'),
cross=10,
showProgress=T,
C=cost)
accC<-c(accC,model@cvResult@ACC)  # save the accuracy value from 10-fold cv
aucC<-c(aucC,model@cvResult@AUC)  # save the area under the curve value from 10-fold cv
}
names(accC)<-costs
names(aucC)<-costs
auc[[k]]<-aucC
acc[[k]]<-accC
# just plotting limits
auc_min<-min(auc_min,aucC)
auc_max<-max(auc_max,aucC)
acc_min<-min(acc_min,accC)
acc_max<-max(acc_max,accC)
}
## Plot the AUC
plot(costs,auc[[1]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,10),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
for(i in 2:5)
{lines(costs,auc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=1:5,col=rainbow(5),pch='l')
## Plot the Accuracy
plot(costs,acc[[1]],col=rainbow(5)[1],ylim=c(acc_min,acc_max),type='l',xlim=c(0,10),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - wo flanks")
for(i in 2:5)
{lines(costs,acc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=1:5,col=rainbow(5),pch='l')
acc[[2]]
acc[[1]]
acc
plot(costs,auc[[3]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,10),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,auc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=1:5,col=rainbow(5),pch='l')
## Plot the Accuracy
plot(costs,acc[[3]],col=rainbow(5)[1],ylim=c(acc_min,acc_max),type='l',xlim=c(0,10),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,acc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=1:5,col=rainbow(5),pch='l')
## Plot the AUC
plot(costs,auc[[3]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,10),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,auc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=3:5,col=rainbow(5)[3:5],pch='l')
## Plot the Accuracy
plot(costs,acc[[3]],col=rainbow(5)[1],ylim=c(acc_min,acc_max),type='l',xlim=c(0,10),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,acc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=3:5,col=rainbow(5)[3:5],pch='l')
legend("bottomright",legend=3:5,col=rainbow(5)[c(1,4,5)],pch='l')
plot(costs,auc[[3]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,10),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,auc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=3:5,col=rainbow(5)[c(1,4,5)],pch='l')
## Plot the Accuracy
plot(costs,acc[[3]],col=rainbow(5)[1],ylim=c(acc_min,acc_max),type='l',xlim=c(0,10),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,acc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=3:5,col=rainbow(5)[c(1,4,5)],pch='l')
accf<-list()
accf_min<-3
accf_max<-0
aucf<-list()
aucf_min<-3
aucf_max<-0
ks<-c(3,4)
costs<-c(0.5,0.75,1,2)
for (k in ks)
{
accC<-c()# save the accuracy value from 10-fold cv
aucC<-c()# save the area under the curve value from 10-fold cv
for (cost in costs)
{
cat(paste(k,'-',cost,'\n'))
specK <- spectrumKernel(k=k)
model <- kbsvm(
x=dataset_flanks$seq,
y=dataset_flanks$lables,
kernel=specK,
pkg="e1071",
svm="C-svc",
perfParameters=c('ACC','AUC'),
cross=10,
showProgress=T,
C=cost)
accC<-c(accC,model@cvResult@ACC)
aucC<-c(aucC,model@cvResult@AUC)
}
names(accC)<-costs
names(aucC)<-costs
aucf[[k]]<-aucC
accf[[k]]<-accC
aucf_min<-min(aucf_min,aucC)
aucf_max<-max(aucf_max,aucC)
accf_min<-min(accf_min,accC)
accf_max<-max(accf_max,accC)
}
plot(costs,aucf[[3]],col=rainbow(5)[1],ylim=c(aucf_min,aucf_max),type='l',xlim=c(0,10),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - with flanks")
plot(costs,accf[[1]],col=rainbow(5)[1],ylim=c(accf_min,accf_max),type='l',xlim=c(0,10),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - with flanks")
plot(costs,accf[[3]],col=rainbow(5)[1],ylim=c(accf_min,accf_max),type='l',xlim=c(0,10),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - with flanks")
plot(costs,auc[[3]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,6),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,auc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=3:5,col=rainbow(5)[c(1,4,5)],pch='l')
plot(costs,auc[[3]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,6),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
costs<-c(0.5,0.75,1,2,6) # the different costs we use
plot(costs,auc[[3]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,6),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,auc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=3:5,col=rainbow(5)[c(1,4,5)],pch='l')
paste('k=',1:2)
legend("bottomright",legend=paste('k=',3:5),col=rainbow(5)[c(1,4,5)],pch='l')
plot(costs,auc[[3]],col=rainbow(5)[1],ylim=c(auc_min,auc_max),type='l',xlim=c(0,6),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,auc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=paste('k=',3:5),col=rainbow(5)[c(1,4,5)],pch='l')
plot(costs,acc[[3]],col=rainbow(5)[1],ylim=c(acc_min,acc_max),type='l',xlim=c(0,6),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - wo flanks")
for(i in 4:5)
{lines(costs,acc[[i]],col=rainbow(5)[i])}
legend("bottomright",legend=paste('k=',3:5),col=rainbow(5)[c(1,4,5)],pch='l')
costs<-c(0.5,0.75,1,2)
plot(costs,aucf[[3]],col=rainbow(5)[1],ylim=c(aucf_min,aucf_max),type='l',xlim=c(0,2),ylab='AUC- Area under the curve', main="AUC's in dependence of costs and kmer size - with flanks")
plot(costs,accf[[3]],col=rainbow(5)[1],ylim=c(accf_min,accf_max),type='l',xlim=c(0,2),ylab='Accuracy', main="Accuracy in dependence of costs and kmer size - with flanks")
sort(abs(basic_model@featureWeights),decreasing=T)
basic_model@featureWeights
basic_model@featureWeights[order(abs(basic_model@featureWeights),decreasing=T)]
names(basic_model@featureWeights)[order(abs(basic_model@featureWeights),decreasing=T)]
basic_model@featureWeights
class(basic_model@featureWeights)
dim(basic_model@featureWeights)
as.numeric(basic_model@featureWeights)
colnames(basic_model@featureWeights)
s<-order(abs(basic_model@featureWeights),decreasing=T)
vipkmer<- basic_model@featureWeights[s]
names(vipkmer)<-colnames(basic_model@featureWeights)[s]
print(vipkmer)
